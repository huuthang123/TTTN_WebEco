import numpy as np
import re
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import KeyedVectors

nltk.download('punkt')

# ===== Config =====
WORD_VECTOR_FILE = 'data/fasttext/en.vec'  # ƒê∆∞·ªùng d·∫´n ƒë·∫øn m√¥ h√¨nh vector
TEXT_FILE = 'data/sample.txt'              # File ch·ª©a review
TOP_N = 3                                   # S·ªë c√¢u t√≥m t·∫Øt

# ===== Cleaning =====
def clean_text(text):
    # X√≥a HTML tags, markdown bullets, emoji, k√Ω t·ª± ƒë·∫∑c bi·ªát, th·ª´a kho·∫£ng tr·∫Øng
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[-‚Äì‚Äî‚Ä¢*]+', '', text)
    text = re.sub(r'[^\w\s.,!?]', '', text)  # Lo·∫°i b·ªè emoji & k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def capitalize_sentences(text):
    # Vi·∫øt hoa ƒë·∫ßu m·ªói c√¢u
    return " ".join(s.capitalize() for s in sent_tokenize(text))

# ===== Load word vectors =====
print("üîÅ Loading word vectors...")
model = KeyedVectors.load_word2vec_format(WORD_VECTOR_FILE)
print("‚úÖ Word vectors loaded.")

# ===== Load and clean text =====
with open(TEXT_FILE, encoding='utf-8') as f:
    raw_text = f.read()

cleaned_text = capitalize_sentences(clean_text(raw_text))
sentences = [s.strip() for s in sent_tokenize(cleaned_text) if len(s.split()) >= 3]

# ===== Sentence Embedding =====
def sentence_vector(sentence):
    words = word_tokenize(sentence.lower())
    vecs = [model[word] for word in words if word in model]
    if not vecs:
        return np.zeros(model.vector_size)
    return np.mean(vecs, axis=0)

sentence_vectors = [sentence_vector(s) for s in sentences]

# ===== Check all vectors =====
if all((vec == 0).all() for vec in sentence_vectors):
    print("‚ö†Ô∏è All sentence vectors are empty. Check your input or vector model.")
    exit()

# ===== Similarity & Ranking =====
sim_matrix = cosine_similarity(sentence_vectors)
scores = sim_matrix.sum(axis=1)
top_idx = np.argsort(scores)[-TOP_N:]
top_idx.sort()

# ===== Output Summary =====
print("\nüìÑ Summary:\n")
for idx in top_idx:
    print(f"- {sentences[idx]}")
